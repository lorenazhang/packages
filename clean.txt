from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# Load the tokenizer and model
model_name = "google/flan-t5-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Define a function to summarize a chunk of text
def summarize_text(text, max_length=100, min_length=30):
    inputs = tokenizer.encode("summarize: " + text, return_tensors="pt", truncation=True)
    summary_ids = model.generate(inputs, max_length=max_length, min_length=min_length, length_penalty=2.0, num_beams=4)
    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)

# Chunk the document into smaller parts
def chunk_document(document, chunk_size=512):
    tokens = tokenizer.encode(document)
    for i in range(0, len(tokens), chunk_size):
        yield tokenizer.decode(tokens[i:i + chunk_size])

# Process the long document
def summarize_long_document(document, chunk_size=512, max_length=100, min_length=30):
    chunks = list(chunk_document(document, chunk_size=chunk_size))
    summaries = [summarize_text(chunk, max_length=max_length, min_length=min_length) for chunk in chunks]
    combined_summary = " ".join(summaries)
    return summarize_text(combined_summary)  # Optionally, summarize the combined summary

# Example usage
long_document = """Your very long document text goes here. This could be several paragraphs or pages long. 
The content will be chunked and summarized effectively."""

final_summary = summarize_long_document(long_document, chunk_size=512, max_length=100, min_length=30)
print("Final Summary:")
print(final_summary)
